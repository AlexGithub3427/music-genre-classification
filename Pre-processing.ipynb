{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4a6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob # list out all files in a directory\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b8274",
   "metadata": {},
   "source": [
    "## 1. Setting directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8d158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob(\"/Users/mayawiegand/Documents/ECS 171/Project/music-genre-classification/Raw Audio Data/*/*.wav\") # creating a list of all of the audio files for all of the genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a398c18",
   "metadata": {},
   "source": [
    "## 2. Reading in raw audio data and converting to mel-spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018098ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 999\n",
      "Failed: 1\n",
      "First failure: ('/Users/mayawiegand/Documents/ECS 171/Project/music-genre-classification/Raw Audio Data/jazz/jazz.00054.wav', 'LibsndfileError(1, \"Error opening \\'/Users/mayawiegand/Documents/ECS 171/Project/music-genre-classification/Raw Audio Data/jazz/jazz.00054.wav\\': \")')\n"
     ]
    }
   ],
   "source": [
    "# initializing containers so we can keep track of how many audio files fail to load and therefore don't get processed\n",
    "ok = 0\n",
    "bad = []\n",
    "\n",
    "# initializing lists to store all of the spectrograms that are created in the loop and their corresponding genre labels and original file paths\n",
    "spectro_list = []\n",
    "genre_list = []\n",
    "paths_list = []\n",
    "\n",
    "for audio in audio_files:\n",
    "    try:\n",
    "        y, sr = sf.read(audio) # using sound file to read in audio, y = raw audio data and sr = sampling rate (how often the audio is sampled by the computer since it isn't continuous like human ears hear it)\n",
    "        if sr != 22050:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=22050) # resampling to 22050 to make sure all files have consistent sampling rate\n",
    "            sr = 22050\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128) # creating mel spectrogram, n_mels = how many perceptual frequency bands do you want (how finely to slice frequency axis to best represent how humans hear it)\n",
    "        S_db_mel = librosa.power_to_db(S, ref=np.max) # converting to log decibels (so this can be understood as volume)\n",
    "        spectro_list.append(S_db_mel) # adding this spectrogram to the list\n",
    "\n",
    "        current_genre = Path(audio).parent.name # grabbing the folder name of the parent folder which is the genre label\n",
    "        genre_list.append(current_genre) # adding this to genre list\n",
    "\n",
    "        current_path = Path(audio) # grabbing the full file path (just in case we need later)\n",
    "        paths_list.append(current_path) # adding this to file path list\n",
    "        \n",
    "        ok += 1\n",
    "    except Exception as e:\n",
    "        bad.append((audio, repr(e)))\n",
    "\n",
    "# printing out the number of audio files that could be successfully loaded and processed, and the number that failed\n",
    "# keeping track of the audio files that failed\n",
    "\n",
    "print(\"Loaded:\", ok)\n",
    "print(\"Failed:\", len(bad))\n",
    "if bad:\n",
    "    print(\"First failure:\", bad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f02ea7",
   "metadata": {},
   "source": [
    "## 3. Splitting data into training and testing set\n",
    "- Need to split into training and testing before audio clips are split into smaller segments (to give us more training instances and hopefully improve model) to prevent data leakage\n",
    "- Audio data typically uses y to represent the audio data - stayed consistent above with this\n",
    "- Now that training and testing datasets are being built, stayed consistent with ML:\n",
    "    - y = genre label\n",
    "    - X = mel-spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(spectro_list, genre_list, paths_list, test_size=0.2, random_state=42, stratify=genre_list)\n",
    "# using stratify here so that the genre proportions are consistent \n",
    "# train/test split is done separately within each class to make sure each genre is represented proportionately in training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a371c45",
   "metadata": {},
   "source": [
    "## 4. Cleaning\n",
    "- Segment into smaller clips (3-5 seconds) and ensuring clips are all the same length\n",
    "- Standardize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ecs171-env)",
   "language": "python",
   "name": "ecs171-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
