{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4a6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob # list out all files in a directory\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b8274",
   "metadata": {},
   "source": [
    "## 1. Setting directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8d158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = glob(\"/Users/mayawiegand/Documents/ECS 171/Project/music-genre-classification/Raw Audio Data/*/*.wav\") # creating a list of all of the audio files for all of the genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276310f",
   "metadata": {},
   "source": [
    "# need to train and test split on just the paths and genre list\n",
    "\n",
    "copy some of the structure below, create the genre and paths list then split into the training/testing sets based on this\n",
    "\n",
    "then segment\n",
    "\n",
    "then make the spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d0e62",
   "metadata": {},
   "source": [
    "## 2. Grabbing genre labels and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e66beaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing lists to store genre labels and original file paths\n",
    "\n",
    "genre_list = []\n",
    "paths_list = []\n",
    "\n",
    "for audio in audio_files:\n",
    "    current_genre = Path(audio).parent.name # grabbing the folder name of the parent folder which is the genre label\n",
    "    genre_list.append(current_genre) # adding this to genre list\n",
    "\n",
    "    current_path = Path(audio) # grabbing the full file path (just in case we need later)\n",
    "    paths_list.append(current_path) # adding this to file path list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030ae91",
   "metadata": {},
   "source": [
    "## 3. Splitting data into training and testing set\n",
    "- Need to split into training and testing before audio clips are split into smaller segments (to give us more training instances and hopefully improve model) to prevent data leakage\n",
    "- Audio data typically uses y to represent the audio data - stayed consistent above with this\n",
    "- Now that training and testing datasets are being built, stayed consistent with ML:\n",
    "    - y = genre label\n",
    "    - X = path to each of the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341294bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genre list/paths list for training/testing split so don't have to load all audio files, then reload to create spectrograms\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(paths_list, genre_list, test_size=0.2, random_state=42, stratify=genre_list)\n",
    "\n",
    "# using stratify here so that the genre proportions are consistent \n",
    "# train/test split is done separately within each class to make sure each genre is represented proportionately in training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf3112",
   "metadata": {},
   "source": [
    "## 4. Segmenting Audo Files, Re-Sampling, and Creating Spectrograms\n",
    "- Segmenting to give us more training observations\n",
    "- Needs to be done after training/testing split to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectro(audio_list, label_list):\n",
    "    \n",
    "    spectro_list = []\n",
    "    clip_labels = []\n",
    "\n",
    "    target_sr = 22050\n",
    "    clip_seconds = 3\n",
    "\n",
    "    samples_per_clip = clip_seconds * target_sr\n",
    "\n",
    "    for audio, label in zip(audio_list, label_list):\n",
    "\n",
    "        y, sr = sf.read(audio) # using sound file to read in audio, y = raw audio data and sr = sampling rate (how often the audio is sampled by the computer since it isn't continuous like human ears hear it)\n",
    "        \n",
    "        # converting to one audio channel (need one-dimensional to create the spectrogram)\n",
    "        if y.ndim == 2:\n",
    "            y = y.mean(axis=1)\n",
    "        \n",
    "        y = y.astype(float) # need y to be a float when computing spectrogram\n",
    "\n",
    "        if sr != 22050:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=22050) # resampling to 22050 to make sure all files have consistent sampling rate\n",
    "            sr = target_sr\n",
    "\n",
    "        num_clips = len(y) // samples_per_clip # grabbing the total number of clips that will be created based on the length and number of audio files\n",
    "\n",
    "        for i in range(num_clips): # looping over the range of clips to create each ckip\n",
    "            start = i * samples_per_clip \n",
    "            end = start + samples_per_clip\n",
    "            clip = y[start:end] # slicing y based on the start and end point to create a clip for this song\n",
    "\n",
    "            S = librosa.feature.melspectrogram(y=clip, sr=sr, n_mels=128) # creating mel spectrogram, n_mels = how many perceptual frequency bands do you want (how finely to slice frequency axis to best represent how humans hear it)\n",
    "            S_db_mel = librosa.power_to_db(S, ref=np.max) # converting to log decibels (so this can be understood as volume)\n",
    "            \n",
    "            spectro_list.append(S_db_mel) # adding this spectrogram to the list\n",
    "            clip_labels.append(label)\n",
    "\n",
    "    return spectro_list, clip_labels\n",
    "\n",
    "X_train_spec, y_train_labels = create_spectro(X_train, y_train)\n",
    "X_test_spec, y_test_labels = create_spectro(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a371c45",
   "metadata": {},
   "source": [
    "## 4. Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283568cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using (x-mean of training)/sd of training to standardize both training and testing spectrograms\n",
    "\n",
    "X_train = np.array(X_train_spec)\n",
    "X_test = np.array(X_test_spec)\n",
    "\n",
    "mu = X_train.mean()\n",
    "sigma = X_train.std() + 1e-8 # adding 1e^-8 prevents the denominator from being 0 (possible result from numerical precision, and parts across a spectrogram can be constant which could lead to 0/very small variance)\n",
    "\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da422eb",
   "metadata": {},
   "source": [
    "The final X_train, X_test and there corresponding genre labels X_train_labels, y_train_lables are ready for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a3888",
   "metadata": {},
   "source": [
    "## 5. Saving Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a0635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('processed_data/y_train.npy'), PosixPath('processed_data/y_test.npy'), PosixPath('processed_data/X_test.npy'), PosixPath('processed_data/X_train.npy')]\n"
     ]
    }
   ],
   "source": [
    "# converting to numpy arrays\n",
    "X_train = np.array(X_train_spec)\n",
    "X_test = np.array(X_test_spec)\n",
    "y_train = np.array(y_train_labels)\n",
    "y_test = np.array(y_test_labels)\n",
    "\n",
    "save_dir = Path(\"processed_data\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "np.save(save_dir / \"X_train.npy\", X_train)\n",
    "np.save(save_dir / \"X_test.npy\", X_test)\n",
    "np.save(save_dir / \"y_train.npy\", y_train)\n",
    "np.save(save_dir / \"y_test.npy\", y_test)\n",
    "\n",
    "print(list(save_dir.glob(\"*\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ecs171-tf)",
   "language": "python",
   "name": "ecs171-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
